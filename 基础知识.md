# 基础知识

1. 什么是motion blur，怎么实现？

   [OpenCV | Motion Blur in Python - GeeksforGeeks](https://www.geeksforgeeks.org/opencv-motion-blur-in-python/)

2. 傅里叶变换的理解

   [【官方双语】形象展示傅里叶变换_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1pW411J7s8?from=search&seid=16994499023389480183)

   从音轨分离的角度切入，很形象。

   傅里叶变化其实就是一个在信号频率上的变换转换到另一个领域。

   首先是直角坐标系，我们想象一个正余弦的变换。记录每一点到横轴的垂直距离h，然后把这个正余弦波摘下来，绕成一个圈，以原点为中心进行缠绕，垂直距离h就是这个圈上的点到原点的距离，我们想象这个圈圈是有质量的，那这个有质量的物体就有一个质心，我们将这个质心离原点的距离（横坐标）与频率（也就是"缠绕频率"）的关系记录下来，就得到了另一个”域“的图像。其实就是呢，**频域到时域的转换**。

   其实上面描述的过程就是almost-fourier transform（近傅里叶变换），在近傅里叶变换前后，两个波形，先进行aft后叠加，还是叠加后进行aft，结果都是一样的。

   以上是视频中对傅里叶变换给出的内容。

   比较直接的对傅里叶进行描述，就是一个周期函数or非周期函数（曲线下面积是有限的）都可用正弦、余弦\*加权函数的积分表示。后者表示出的feature和前者一模一样，不会有任何损失。

    暂时不要纠结快速傅里叶变换，就是一种用o(nlogn)的复杂度去进行傅里叶变换的方法。以后有需要的话再仔细纠结。

3. **half-quadratic splitting algorithm**

   [1704.03264v1.pdf (arxiv.org)](https://arxiv.org/pdf/1704.03264v1.pdf)

   在这篇文章中有提到相关的算法，Kai Zhang真是个神仙。

   HQS是一种参数分离的方法。可以设置噪声先验的模块，这样我们就有个一个denoiser。那什么叫噪声先验模块呢？一直都觉得图像处理里“prior”这个说法非常奇怪。一般来说，先验指的是在没有做出任何测试时，对某一件事情的认识，“认识”这个概念就过于抽象了。

   这个其实算不上基础知识，应该是超分领域的一个知识点

   integration with fast discriminative denoiser prior

   这篇论文提出的方案能够对高斯噪音进行很好的“还原”，也可以作为一种prior（这个部分就不懂了）

   *image restoration和image super resolution的区别*

   其实我觉得没啥区别。。。前者包含的范围可能要广一些

   image denoising(算子是identity matrix), image deblurring（算子是blurring operator）, image super resolution（算子是composite operator of blurring and down-sampling）（前者包含)

   prior==regularization（看下面的解释，确实是一种正则化）再详细一下可以看4中视频的推导。其实MAP就是加了正则化的MLE

   这篇文章主要1. 训练出了很棒的denoiser，其中使用到了参数分离技术，也就是我们最开始提到的HQS，这些denoiser的prior效果很nice 2. 还是将model和learning方法结合，在保证精度的同时有没有丢失灵活性。

   ##### background

   提出了ADMM和HQS这样的变量分离法得到了很多的应用。

   对HQS进行了详细的解释，我们看map的推导过程，虽然写成了一个fidelity term和regularization term相加的形式，但是这两个term之间有关联，无法分开求解，HQS做的就是把这两个term完完全全地分隔开，听起来很神奇。

   在这里不写了，只做注释，可以看论文的2.2节。

   他直接将regularization term中的变量x换成了z，但是这样肯定是不行的，所以在上式基础上加了一个penalty parameter，然后求这个的最小值。

   我也不知道是怎么**iteratively solute**这个方程的，反正就通过迭代求解求到了这个方程的解（这个解能够使得lossfunction最小）

   需要好多的数学背景知识啊，以及怎样求解一个平方和最小问题。

   当然如果数学知识到位了，这边就很好理解了

   ##### learning deep CNN denoiser prior

   因为在上一小节中提到过，可以用denoiser prior去代替image prior，这样说不定还会获得更好的效果。

   然后给出了用CNN去训练得到的denoiser中CNN的结构图

   **使用加宽的filter去扩大感受域（不就是增大filter size吗）** 

   这个操作的依据是可以更好地利用context information

   增大receptive field有两个角度：filter size，depth，但是增大filter size的代价太大了，所以我们选择增加depth。但在本文中我们选择在这两者之间做一个tradeoff。

   Dilated convolution空洞卷积，通过这种方式来增加receptive field的大小

   [Dilated Convolution | A Blog From Human-engineer-being A Blog From Human-engineer-being (erogol.com)](https://erogol.com/dilated-convolution/)

   **使用BN和residual learning来加速训练过程**

   大概夸赞了一波这两个东西多么的有效果

   ##### 为了避免restoration的效果过于人工化，使用training samples with small size

   symmetric padding and zero padding，由于边缘可能会失真，所以可以通过添加padding来解决。

   然后我们发现使用相对较小的patch，boundary information可以被强化，所以会有更好的提取效果。

   又使用了快速傅里叶变换的思想，然后假设是circular boundary condition。（我暂时还不太明白

   对于单张图像超分来说，现有的超分模型主要着力于model the prior knowledge。are trained for specific degradation process。

   一些对未来的展望（虽然这篇文章是比较早的了

   reduce the number of discriminative cnn denoisers and the number of whole iterations.

   imply the proposed cnn denoiser based hqs to other inverse problem

   utilizing multiple priors

   cnn has its own flexibility, although MAP inference get a pretty good implement here, it's not totally necessary.

4. **MAP与MLE**

   在另一篇论文中有一定的描述但是有很多有疑问的地方，这个视频讲的很好：

   [【机器学习我到底在学什么】哲学角度聊聊贝叶斯派和频率派，数学角度看看极大似然估计和最大后验估计_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Ea4y1J7Jq?from=search&seid=4367632565342926883)

   就是拿抛硬币举例子，我做了十次实验，正面朝上的次数是9次，这个随机变量记为X,那P(X)=0.9，我们假设随机变量满足参数为theta的分布，那么**P(theta|X)=P(X|theta)\*P(theta)/P(X)**,借助最大似然估计的思想，我们希望这个概率最大，由于P(X)是完全确定的，所以我们只需要考虑**P(X|theta)\*P(theta)**最大，这个时候可以跟最大似然估计对比，最大似然估计的表达是使得P(X|theta)最大，可以看出前者比后者多出了一个P(theta)，其实也就是先验。
   然后继续从0.9开始，这个时候需要先复习一下最大似然估计。[参数估计(二).最大似然估计 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/55791843) 

   > 在不同的时候， ![[公式]](https://www.zhihu.com/equation?tex=p%28x%7C%5Ctheta%29) 可以表示概率也可以用于计算似然，这里给出个人的理解，整理如下：
   >
   > - 在 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 已知，![[公式]](https://www.zhihu.com/equation?tex=x) 为变量的情况下，![[公式]](https://www.zhihu.com/equation?tex=p%28x%7C%5Ctheta%29) 为概率，表示通过已知的分布函数与参数，随机生成出 ![[公式]](https://www.zhihu.com/equation?tex=x) 的概率；
   > - 在 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 为变量，![[公式]](https://www.zhihu.com/equation?tex=x) 已知的情况下，![[公式]](https://www.zhihu.com/equation?tex=p%28x%7C%5Ctheta%29) 为似然函数，它表示对于不同的 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) ，出现 ![[公式]](https://www.zhihu.com/equation?tex=x) 的概率是多少。此时可写成 ![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctheta%7Cx%29%3Dp%28x%7C%5Ctheta%29) ，更严格地，我们也可写成 ![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctheta%7Cx%29%3Dp%28x%3B%5Ctheta%29) 。

   然后我们继续上面的话题，最大后验估计使P(theta|X)最大，在这里X是已知的，也就是在一个已知结果下，theta的值取多少能够使得在这个结果下，这个分布的概率最大，P(X|theta)\*P(theta)的乘积最大。（其实可能等式左边的那个条件概率根本没什么意义，只是人们发现使这个部分最大可以很好地进行参数估计）

   那需要思考的有意思的部分就是，MLE中，只是使得前面一部分最大，这个很好理解，就是求theta在取什么值的时候，这个已经发生了的事情发生的概率最大。而MAP会更全面一些，因为还有乘了一个prior，就是上面抛硬币的那个例子，虽然得到的P(X)为0.9，基于这个事实，一个从来没见过硬币的人去估计这个概率（MLE）跟一个一直制造硬币的人去估计这个概率（MAP）的结果显然是不一样的。这也是为什么会觉得MAP是更准确更靠谱的。

5. **Transformer**

   是一种神经网络架构。因为他一开始被用在机器翻译中，就像是一个transformer。

   为什么transformer效果好呢？

   the vanilla transformer  [Attention is All you Need (nips.cc)](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

   transformer综述 https://arxiv.org/pdf/2106.04554.pdf

   #### self-attention and transformer

   如何设计出更恰当的模型；在既有模型的情况下，如何更好地利用数据

   对语言的认识有点像编译原理。

   *如何对语言的context进行建模？*

   早期：卷积网络；sequential model

   如何对长程依赖进行直接的model：全连接

   下一层的任何一个元素可以对上一层的任意元素进行直接的依赖

   但是这个只能去model固定字数的句子，这个是不够灵活的

   所以为了使得能够比较灵活地model句子，我们保证weights是动态的。这也就是所谓的transformer。

   他看起来就是一个围成圈的全连接图

![img](https://i.loli.net/2021/06/28/Gsn2FBT6JYvhSlA.png)

什么是注意力机制？

先计算注意力的**分布**；再依据这个分布对输入的信息进行加权；

KQV模型，因为attention的本质是计算“重要程度”，KQA模型的思想就是计算query和key的相似度进行寻址，这个相似度就反应出了对应Value的“重要程度”。

self-attention是建立在上面这个东东的基础上的，它的query=key=value.

文本自己跟自己求相似度，没有另外的字典。即自己跟上下文求相似度。

传统RNN其实可以捕捉一定的长距离依赖信息，但是随着文本越来越长，捕捉依赖关系的能力越来越低。使用attention则可以减缓这方面的问题。

6. **一些图像处理领域的概念理解**

   **filter**：抑制或最小化某些频率的波或震荡的装置或材料

   这个是图像里对滤波器最初的解释，而后在CNN中，filter指代了卷积核组（kernel的上一维的东西）

   **频率**：自变量单位变化期间，一个周期函数重复相同值序列的次数

7. Fisher information Matrix

   前提：我们有一个参数vector为theta的model。这个模型记录了这样的一个分布，我们把它记作p(x|theta)，然后我们对这个分布取log，加梯度，得到score function。

   [Fisher Information Matrix - Agustinus Kristiadi's Blog (wiseodd.github.io)](https://wiseodd.github.io/techblog/2018/03/11/fisher-information/)

   他可以用来估计似然函数的方差，也可以用来估计mle二阶导数的期望

   

